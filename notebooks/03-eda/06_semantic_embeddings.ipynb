{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2fe5d96d-db41-43b7-a738-b7f54ad17ef4",
            "metadata": {},
            "source": [
                " # Notebook 7: Semantic Embeddings\n",
                "\n",
                " ## Purpose\n",
                " - Generate BERT embeddings for articles\n",
                " - Calculate semantic similarity scores\n",
                " - Demographic group distance in embedding space\n",
                " - Clustering analysis\n",
                " - Dimensionality reduction (PCA/UMAP)\n",
                "\n",
                " ## Outputs\n",
                " - `data/processed/embeddings.h5`\n",
                " - `data/processed/semantic_features.parquet`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "c8a2479d-87de-41d5-948e-440d9b72b42f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/barunakumar/code-ground/news-bias-analyzer/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Libraries imported\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "import torch\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.cluster import KMeans\n",
                "import h5py\n",
                "\n",
                "print(\"Libraries imported\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "510e078b-3d5c-4624-a8b8-55ba7d1491b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "BASE_DIR = Path('..')\n",
                "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
                "INPUT_FILE = PROCESSED_DIR / 'articles_with_events.csv'\n",
                "EMBEDDING_FILE = PROCESSED_DIR / 'embeddings_sample.h5'\n",
                "OUTPUT_FILE = PROCESSED_DIR / 'semantic_features.parquet'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "4996c064-ccf4-4c0c-9be1-06363d17905c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data...\n",
                        "Loaded 49,926 articles\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/var/folders/9s/6vlynz8d6hzbq58p0sbxfjc174vfgt/T/ipykernel_19896/2689917194.py:2: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                        "  df = pd.read_csv(INPUT_FILE)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading data...\")\n",
                "df = pd.read_csv(INPUT_FILE)\n",
                "print(f\"Loaded {len(df):,} articles\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7338873c-50b9-4c41-b69b-5402d83924d2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading BERT model...\n",
                        "Model loaded on mps\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading BERT model...\")\n",
                "model_name = 'bert-base-uncased'\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModel.from_pretrained(model_name)\n",
                "device = torch.device('mps')\n",
                "model.to(device)\n",
                "print(f\"Model loaded on {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "b17d96f7-9c25-4376-80f1-b2f6cfa57237",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_bert_embedding(text, max_length=512):\n",
                "    '''Generate BERT embedding for text'''\n",
                "    if not text:\n",
                "        return np.zeros(768)\n",
                "    \n",
                "    inputs = tokenizer(text[:1000], return_tensors='pt', \n",
                "                      max_length=max_length, truncation=True, padding=True)\n",
                "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "    \n",
                "    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]\n",
                "    return embedding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "49bf88e1-f26f-410a-9ba2-7b2904ed3df1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating embeddings (this will take time)...\n",
                        "  Processed 1,000/49,926 articles\n",
                        "  Processed 2,000/49,926 articles\n",
                        "  Processed 3,000/49,926 articles\n",
                        "  Processed 4,000/49,926 articles\n",
                        "  Processed 5,000/49,926 articles\n",
                        "  Processed 6,000/49,926 articles\n",
                        "  Processed 7,000/49,926 articles\n",
                        "  Processed 8,000/49,926 articles\n",
                        "  Processed 9,000/49,926 articles\n",
                        "  Processed 10,000/49,926 articles\n",
                        "  Processed 11,000/49,926 articles\n",
                        "  Processed 12,000/49,926 articles\n",
                        "  Processed 13,000/49,926 articles\n",
                        "  Processed 14,000/49,926 articles\n",
                        "  Processed 15,000/49,926 articles\n",
                        "  Processed 16,000/49,926 articles\n",
                        "  Processed 17,000/49,926 articles\n",
                        "  Processed 18,000/49,926 articles\n",
                        "  Processed 19,000/49,926 articles\n",
                        "  Processed 20,000/49,926 articles\n",
                        "  Processed 21,000/49,926 articles\n",
                        "  Processed 22,000/49,926 articles\n",
                        "  Processed 23,000/49,926 articles\n",
                        "  Processed 24,000/49,926 articles\n",
                        "  Processed 25,000/49,926 articles\n",
                        "  Processed 26,000/49,926 articles\n",
                        "  Processed 27,000/49,926 articles\n",
                        "  Processed 28,000/49,926 articles\n",
                        "  Processed 29,000/49,926 articles\n",
                        "  Processed 30,000/49,926 articles\n",
                        "  Processed 31,000/49,926 articles\n",
                        "  Processed 32,000/49,926 articles\n",
                        "  Processed 33,000/49,926 articles\n",
                        "  Processed 34,000/49,926 articles\n",
                        "  Processed 35,000/49,926 articles\n",
                        "  Processed 36,000/49,926 articles\n",
                        "  Processed 37,000/49,926 articles\n",
                        "  Processed 38,000/49,926 articles\n",
                        "  Processed 39,000/49,926 articles\n",
                        "  Processed 40,000/49,926 articles\n",
                        "  Processed 41,000/49,926 articles\n",
                        "  Processed 42,000/49,926 articles\n",
                        "  Processed 43,000/49,926 articles\n",
                        "  Processed 44,000/49,926 articles\n",
                        "  Processed 45,000/49,926 articles\n",
                        "  Processed 46,000/49,926 articles\n",
                        "  Processed 47,000/49,926 articles\n",
                        "  Processed 48,000/49,926 articles\n",
                        "  Processed 49,000/49,926 articles\n",
                        "  Processed 50,000/49,926 articles\n",
                        "Generated 49926 embeddings of shape (49926, 768)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Generating embeddings (this will take time)...\")\n",
                "embeddings_list = []\n",
                "batch_size = 100\n",
                "\n",
                "for i in range(0, len(df), batch_size):\n",
                "    batch = df['article_text'].iloc[i:i+batch_size]\n",
                "    batch_embeddings = [get_bert_embedding(text) for text in batch]\n",
                "    embeddings_list.extend(batch_embeddings)\n",
                "    \n",
                "    if (i+batch_size) % 1000 == 0:\n",
                "        print(f\"  Processed {i+batch_size:,}/{len(df):,} articles\")\n",
                "\n",
                "embeddings = np.array(embeddings_list)\n",
                "print(f\"Generated {len(embeddings)} embeddings of shape {embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "bc98285e-f512-49af-b375-b8a05f30ea1c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saving embeddings to HDF5...\n",
                        "Saved to ../data/processed/embeddings_sample.h5\n"
                    ]
                }
            ],
            "source": [
                "print(\"Saving embeddings to HDF5...\")\n",
                "with h5py.File(EMBEDDING_FILE, 'w') as f:\n",
                "    f.create_dataset('embeddings', data=embeddings)\n",
                "    f.create_dataset('urls', data=df['url'].astype(str).values)\n",
                "print(f\"Saved to {EMBEDDING_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "c10a3db9-26b5-4811-bb93-bc06ab67c0b4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Performing PCA dimensionality reduction...\n",
                        "PCA complete: (49926, 50)\n",
                        "  Explained variance: 69.06%\n"
                    ]
                }
            ],
            "source": [
                "print(\"Performing PCA dimensionality reduction...\")\n",
                "pca = PCA(n_components=50)\n",
                "embeddings_pca = pca.fit_transform(embeddings)\n",
                "print(f\"PCA complete: {embeddings_pca.shape}\")\n",
                "print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "eab732c3-ae1a-481c-938f-737623eb94e5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Performing clustering...\n",
                        "Clustering complete: 10 clusters\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Performing clustering...\")\n",
                "n_clusters = 10\n",
                "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
                "clusters = kmeans.fit_predict(embeddings_pca)\n",
                "df['semantic_cluster'] = clusters\n",
                "print(f\"Clustering complete: {n_clusters} clusters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "61a86315-6c02-4a38-947c-b550b7e4e0dd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calculating semantic features...\n",
                        "Saved semantic features to ../data/processed/semantic_features.parquet\n"
                    ]
                }
            ],
            "source": [
                "print(\"Calculating semantic features...\")\n",
                "cluster_centers = kmeans.cluster_centers_\n",
                "df['distance_to_cluster_center'] = [\n",
                "    np.linalg.norm(embeddings_pca[i] - cluster_centers[clusters[i]])\n",
                "    for i in range(len(embeddings_pca))\n",
                "]\n",
                "\n",
                "# Save semantic features\n",
                "semantic_features = df[['url', 'semantic_cluster', 'distance_to_cluster_center']].copy()\n",
                "semantic_features.to_parquet(OUTPUT_FILE, index=False)\n",
                "print(f\"Saved semantic features to {OUTPUT_FILE}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
